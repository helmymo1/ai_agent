{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SentenceTransformer with Advanced Embedding Models - Complete Guide"
      ],
      "metadata": {
        "id": "MIODE0VutqIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Sentence Embeddings with SentenceTransformer\n",
        "#\n",
        "### This notebook demonstrates how to use the SentenceTransformer library with modern embedding models,\n",
        "### including setup for custom models like GemmaRmbedding and other state-of-the-art models."
      ],
      "metadata": {
        "id": "MIaDlAdatyzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Installation and Setup"
      ],
      "metadata": {
        "id": "Y0GH3kdsuAnq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGCnFbh3ti3m",
        "outputId": "1856da0d-49d7-4a3d-c03b-51b93ce85568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.6)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.3.2)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -U sentence-transformers torch transformers datasets faiss-cpu matplotlib seaborn scikit-learn pandas numpy PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from typing import List, Dict, Union, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# SentenceTransformer imports\n",
        "from sentence_transformers import SentenceTransformer, models, util\n",
        "from sentence_transformers import CrossEncoder, InputExample, losses\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaGU91IotxS6",
        "outputId": "fef564bd-45f4-454e-a8e9-cabd084d4d26"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL8bVwp7wIog",
        "outputId": "7d30752d-f578-44f6-c572-2146ad0a98ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `hf auth whoami` to get more information or `hf auth logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `test` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `test`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Choosing and Loading the Embedding Model"
      ],
      "metadata": {
        "id": "wcbakImMuTgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a top-tier embedding model from Hugging Face\n",
        "# This model is a strong performer on the MTEB leaderboard.\n",
        "model_name = 'google/embeddinggemma-300m'\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "print(f\"Model '{model_name}' loaded successfully.\")\n",
        "print(f\"Max sequence length: {model.max_seq_length}\")\n",
        "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC9Eb8hVutbt",
        "outputId": "f572c424-4284-488b-b2be-e93170f1c7a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 'google/embeddinggemma-300m' loaded successfully.\n",
            "Max sequence length: 2048\n",
            "Embedding dimension: 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Processing the PDF Document"
      ],
      "metadata": {
        "id": "ZL0zWC9HvsiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"Extracts all text from a given PDF file.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    full_text = \"\"\n",
        "    for page in doc:\n",
        "        full_text += page.get_text()\n",
        "    doc.close()\n",
        "    return full_text\n",
        "\n",
        "# Extract text from our dummy paper\n",
        "pdf_path = \"/content/2025 pa.pdf\"\n",
        "paper_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "print(\"--- Extracted Text ---\")\n",
        "print(paper_text[:500] + \"...\") # Print the first 500 characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wjmW-9YwcJt",
        "outputId": "3bd95b85-a2da-4901-c6db-5ba88ab5760d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Extracted Text ---\n",
            "Integrating vision transformer-\n",
            "based deep learning model with \n",
            "kernel extreme learning machine \n",
            "for non-invasive diagnosis of \n",
            "neonatal jaundice using biomedical \n",
            "images\n",
            "M. Eliazer1, Sibi Amaran1, K. Sreekumar1, A. Vikram2, Gyanendra Prasad Joshi3 & \n",
            "Woong Cho3\n",
            "Birth complications, particularly jaundice, are one of the leading causes of adolescent death and \n",
            "disease all over the globe. The main severity of these illnesses may diminish if scholars study more \n",
            "about their sources and progress t...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Text Chunking"
      ],
      "metadata": {
        "id": "daeh37rzw5gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text: str, chunk_size: int = 1024, chunk_overlap: int = 128) -> list[str]:\n",
        "    \"\"\"\n",
        "    Splits a text into overlapping chunks.\n",
        "\n",
        "    Args:\n",
        "        text: The input text.\n",
        "        chunk_size: The desired size of each chunk in characters.\n",
        "        chunk_overlap: The number of characters to overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        A list of text chunks.\n",
        "    \"\"\"\n",
        "    if len(text) <= chunk_size:\n",
        "        return [text]\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(text[start:end])\n",
        "        start += chunk_size - chunk_overlap\n",
        "    return chunks\n",
        "\n",
        "# Chunk the extracted paper text\n",
        "text_chunks = chunk_text(paper_text, chunk_size=800, chunk_overlap=100)\n",
        "\n",
        "print(f\"The text was split into {len(text_chunks)} chunks.\")\n",
        "print(\"\\n--- First Chunk ---\")\n",
        "print(text_chunks[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slwTEMqbw0HA",
        "outputId": "020c1d62-c9e3-4fbc-df9a-9a9e371d5ed0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text was split into 84 chunks.\n",
            "\n",
            "--- First Chunk ---\n",
            "Integrating vision transformer-\n",
            "based deep learning model with \n",
            "kernel extreme learning machine \n",
            "for non-invasive diagnosis of \n",
            "neonatal jaundice using biomedical \n",
            "images\n",
            "M. Eliazer1, Sibi Amaran1, K. Sreekumar1, A. Vikram2, Gyanendra Prasad Joshi3 & \n",
            "Woong Cho3\n",
            "Birth complications, particularly jaundice, are one of the leading causes of adolescent death and \n",
            "disease all over the globe. The main severity of these illnesses may diminish if scholars study more \n",
            "about their sources and progress toward effective treatment. Assured developments were prepared, \n",
            "but they are inadequate. Newborns repeatedly have jaundice as their primary medical concern. A \n",
            "raised level of bilirubin is a symbol of jaundice. Generally, in newborns, hyperbilirubinemia peaks in \n",
            "the initial post-delivery week. The \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Generating the Embeddings\n"
      ],
      "metadata": {
        "id": "uaE7JLX5xGhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate embeddings for each chunk\n",
        "# The model will automatically handle batching.\n",
        "# We can show a progress bar for large numbers of chunks.\n",
        "embeddings = model.encode(text_chunks, show_progress_bar=True)\n",
        "\n",
        "# The result is a NumPy array\n",
        "print(f\"\\nShape of the embeddings array: {embeddings.shape}\")\n",
        "print(f\"This means we have {embeddings.shape[0]} vectors, each with {embeddings.shape[1]} dimensions.\")\n",
        "\n",
        "# You can save these embeddings for later use\n",
        "np.save(\"paper_embeddings.npy\", embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "fdbbeadc36aa48dca3903d68f96fdb68",
            "08f6122e77654fd6adc1e2a22f94424e",
            "252d056f58204204981f004637bbf596",
            "b372c7d96c0445c68d6dbeade43eba7c",
            "24f454a1840a46e081081af7092f9939",
            "ae90aa19bb5b49abb4342fd23cacbe5b",
            "a69c8ebbe6704ed49868c19b8d9a9b03",
            "b1551156cd6e46ef9b17ea2f41787722",
            "637ada80578c479b88acd49fcba12379",
            "0d5d196c276b4611b252d2a108493cba",
            "e5ce38ce4f614baf8c4df133cc9531d8"
          ]
        },
        "id": "E3TDK8DQxAZZ",
        "outputId": "91335092-52f7-4ec7-e794-9f07948e2ba8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdbbeadc36aa48dca3903d68f96fdb68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape of the embeddings array: (84, 768)\n",
            "This means we have 84 vectors, each with 768 dimensions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Example Usage: Semantic Search\n"
      ],
      "metadata": {
        "id": "cCQ_YW9fxO9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.util import cos_sim\n",
        "\n",
        "def search(query: str, chunks: list[str], embeddings: np.ndarray, top_k: int = 2):\n",
        "    \"\"\"\n",
        "    Finds the most relevant chunks for a given query.\n",
        "    \"\"\"\n",
        "    # 1. Embed the query\n",
        "    query_embedding = model.encode(query)\n",
        "\n",
        "    # 2. Calculate cosine similarity between the query and all chunks\n",
        "    similarities = cos_sim(query_embedding, embeddings)[0]\n",
        "\n",
        "    # 3. Find the top_k most similar chunks\n",
        "    # We use torch.topk for efficiency\n",
        "    top_k_indices = np.argsort(similarities)[-top_k:]\n",
        "\n",
        "    # 4. Return the results\n",
        "    results = []\n",
        "    for idx in reversed(top_k_indices): # Show the most similar first\n",
        "        results.append({\n",
        "            \"chunk\": chunks[idx],\n",
        "            \"similarity\": float(similarities[idx])\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# --- Let's test it! ---\n",
        "user_query = \"What is Equation for MSA(z)?\"\n",
        "\n",
        "search_results = search(user_query, text_chunks, embeddings)\n",
        "\n",
        "print(f\"\\nQuery: '{user_query}'\\n\")\n",
        "print(\"--- Top Search Results ---\")\n",
        "for i, result in enumerate(search_results):\n",
        "    print(f\"Result {i+1} (Similarity: {result['similarity']:.4f}):\")\n",
        "    print(result['chunk'])\n",
        "    print(\"-\" * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH-AeSANxJoR",
        "outputId": "ebc5e40e-a443-4ee1-db1c-e1b1eb4c0f93"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: 'What is Equation for MSA(z)?'\n",
            "\n",
            "--- Top Search Results ---\n",
            "Result 1 (Similarity: 0.5014):\n",
            "s any non-commercial use, sharing, distribution and reproduction in \n",
            "any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \n",
            "a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have \n",
            "permission under this licence to share adapted material derived from this article or parts of it. The images or \n",
            "other third party material in this article are included in the article’s Creative Commons licence, unless indicated \n",
            "otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \n",
            "and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \n",
            "obtain permission directly from the copyright holder. \n",
            "-------------------------\n",
            "Result 2 (Similarity: 0.4923):\n",
            "a greater proportion of actual positives (recall). The stable rise in PR values amongst all class \n",
            "labels describes the efficiency of the EDNJIC-KELM technique in the classification procedure.\n",
            "In Fig. 12, the ROC values of the EDNJIC-KELM methodology under 80:20 are examined. The outcomes \n",
            "suggest that the EDNJIC-KELM technique accomplishes superior ROC values over each class, signifying its \n",
            "essential capacity for distinguishing classes. This steady trend of advanced analysis of ROC over many classes \n",
            "Fig. 9.  Accuy curve of the EDNJIC-KELM method under 80:20.\n",
            " \n",
            "Fig. 8.  Average of EDNJIC-KELM model under (a and b) TRAPHA 80% and TESPHA 20%\n",
            " \n",
            "Scientific Reports |        (2025) 15:25493 \n",
            "13\n",
            "| https://doi.org/10.1038/s41598-025-08342-2\n",
            "www.nature.com/scientificreports/\n",
            "signifies the profici\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 🚀 Thank You & Next Steps\n",
        "\n",
        "Thank you for working through this notebook! You've successfully seen how to use a state-of-the-art embedding model for a practical task: processing a PDF and performing a semantic search.\n",
        "\n",
        "We hope this demonstration has been a valuable and practical learning experience.\n",
        "\n",
        "### Explore More with Us\n",
        "\n",
        "If you found this guide helpful, we invite you to explore more of our work. Our team is passionate about pushing the boundaries of AI and sharing our findings with the community.\n",
        "\n",
        "- **Visit Our Website:** Discover our latest projects, research, and the services we offer.\n",
        "  [**[Momen Walied Website]**](https://momenwalied.camitai.com)\n",
        "\n",
        "- **Read Our Blog:** For more deep dives, technical tutorials, and insights into the world of NLP and Large Language Models, be sure to check out our blog.\n",
        "  [**Read Our Latest Blog Posts**](https://momenwalied.camitai.com/blog/PRDs)\n",
        "\n",
        "We are constantly updating our content with new findings. Stay connected with us to keep learning!\n",
        "\n",
        "Happy coding!"
      ],
      "metadata": {
        "id": "TTL6xuDG362h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fdtedSVSDf6r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
